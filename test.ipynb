{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom fastapi import FastAPI, File, UploadFile, Query, HTTPException, Request, Form\\nfrom fastapi.responses import JSONResponse, FileResponse\\nfrom fastapi.encoders import jsonable_encoder\\nimport shutil\\nfrom typing import List\\nimport subprocess\\nfrom osllm import OpenSourceLLM\\nimport nest_asyncio\\nimport uvicorn\\nfrom pydantic import BaseModel\\n\\napp = FastAPI()\\n\\nllm = OpenSourceLLM()\\n@app.get(\"/\")\\ndef read_root():\\n    return {\"message\": \"Estoy vivo!\"}\\n\\nclass Modelo(BaseModel):\\n    nombre: str\\n    descripcion: str\\n\\nclass OpcionesModelo(BaseModel):\\n    modelos: List[Modelo]\\n\\n@app.post(\"/uploadfile/\")\\nasync def create_upload_file(file: UploadFile = File(...),speakers: str = Query(..., title=\"Speaker Name\", description=\"Name of the speaker\"),\\n                             language: str = Query(..., title=\"Language\", description=\"Language for speech-to-text\")):\\n    try:\\n        if file.filename.endswith(\\'.mp3\\'):\\n            with open(f\"/workspace/{file.filename}\", \"wb\") as mp3_file:\\n                shutil.copyfileobj(file.file, mp3_file)\\n            #subprocess.run([\"python\",\"audio2text.py\",f\"uploaded_files/{file.filename}\",speakers,language])\\n            return JSONResponse(content=jsonable_encoder({\"message\": f\"File uploaded successfully, speakers= {speakers}, language={language}\"}), status_code=200)\\n        else:\\n            return JSONResponse(content=jsonable_encoder({\"error\": \"Only MP3 files are allowed\"}), status_code=400)\\n    except Exception as e:\\n        return JSONResponse(content=jsonable_encoder({\"error\": str(e)}), status_code=500)\\n\\n@app.get(\"/speech-to-text/\")\\nasync def speech_to_text(speakers: str = Query(..., title=\"Speaker Name\", description=\"Name of the speaker\"),\\n                         language: str = Query(..., title=\"Language\", description=\"Language for speech-to-text\")):\\n    response_content = {\\n        \"message\": f\"speech-to-text conversion for {speakers} in {language} language.\"\\n    }\\n    return response_content\\n\\n@app.get(\"/get-transcript/\")\\nasync def get_text_file(name_file: str = Query(..., title=\"Nombre del archivo\", description=\"Nombre del archivo\")):\\n    file_path = f\"/workspace/transcript.txt\"\\n    transcription_processor.run_transcription(\\'/workspace/\\' + name_file)\\n    return FileResponse(file_path, filename=f\"transcript.txt\", media_type=\"text/plain\")\\n\\n@app.get(\"/modelos\", response_model=OpcionesModelo)\\nasync def obtener_modelos():\\n    opciones = []\\n    for nombre, descripcion in OpenSourceLLM.OPCIONES_MODELOS.items():\\n        opciones.append(Modelo(nombre=nombre, descripcion=descripcion))\\n    return OpcionesModelo(modelos=opciones)\\n\\n@app.post(\"/generar_texto\", response_model=str)\\nasync def generar_texto(request: Request, form_data: Form(...)):\\n    modelo_elegido = form_data.modelo\\n    texto = form_data.texto\\n\\n    llm = OpenSourceLLM(modelo_elegido)\\n    respuesta = llm.text2text(texto)\\n\\n    return respuesta\\n\\nnest_asyncio.apply()\\n\\nconfig = uvicorn.Config(app=app, host=\"127.0.0.1\", port=8000, loop=\\'asyncio\\')\\nserver = uvicorn.Server(config)\\nawait server.serve()\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from fastapi import FastAPI, File, UploadFile, Query, HTTPException, Request, Form\n",
    "from fastapi.responses import JSONResponse, FileResponse\n",
    "from fastapi.encoders import jsonable_encoder\n",
    "import shutil\n",
    "from typing import List\n",
    "import subprocess\n",
    "from osllm import OpenSourceLLM\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "llm = OpenSourceLLM()\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    return {\"message\": \"Estoy vivo!\"}\n",
    "\n",
    "class Modelo(BaseModel):\n",
    "    nombre: str\n",
    "    descripcion: str\n",
    "\n",
    "class OpcionesModelo(BaseModel):\n",
    "    modelos: List[Modelo]\n",
    "\n",
    "@app.post(\"/uploadfile/\")\n",
    "async def create_upload_file(file: UploadFile = File(...),speakers: str = Query(..., title=\"Speaker Name\", description=\"Name of the speaker\"),\n",
    "                             language: str = Query(..., title=\"Language\", description=\"Language for speech-to-text\")):\n",
    "    try:\n",
    "        if file.filename.endswith('.mp3'):\n",
    "            with open(f\"/workspace/{file.filename}\", \"wb\") as mp3_file:\n",
    "                shutil.copyfileobj(file.file, mp3_file)\n",
    "            #subprocess.run([\"python\",\"audio2text.py\",f\"uploaded_files/{file.filename}\",speakers,language])\n",
    "            return JSONResponse(content=jsonable_encoder({\"message\": f\"File uploaded successfully, speakers= {speakers}, language={language}\"}), status_code=200)\n",
    "        else:\n",
    "            return JSONResponse(content=jsonable_encoder({\"error\": \"Only MP3 files are allowed\"}), status_code=400)\n",
    "    except Exception as e:\n",
    "        return JSONResponse(content=jsonable_encoder({\"error\": str(e)}), status_code=500)\n",
    "\n",
    "@app.get(\"/speech-to-text/\")\n",
    "async def speech_to_text(speakers: str = Query(..., title=\"Speaker Name\", description=\"Name of the speaker\"),\n",
    "                         language: str = Query(..., title=\"Language\", description=\"Language for speech-to-text\")):\n",
    "    response_content = {\n",
    "        \"message\": f\"speech-to-text conversion for {speakers} in {language} language.\"\n",
    "    }\n",
    "    return response_content\n",
    "\n",
    "@app.get(\"/get-transcript/\")\n",
    "async def get_text_file(name_file: str = Query(..., title=\"Nombre del archivo\", description=\"Nombre del archivo\")):\n",
    "    file_path = f\"/workspace/transcript.txt\"\n",
    "    transcription_processor.run_transcription('/workspace/' + name_file)\n",
    "    return FileResponse(file_path, filename=f\"transcript.txt\", media_type=\"text/plain\")\n",
    "\n",
    "@app.get(\"/modelos\", response_model=OpcionesModelo)\n",
    "async def obtener_modelos():\n",
    "    opciones = []\n",
    "    for nombre, descripcion in OpenSourceLLM.OPCIONES_MODELOS.items():\n",
    "        opciones.append(Modelo(nombre=nombre, descripcion=descripcion))\n",
    "    return OpcionesModelo(modelos=opciones)\n",
    "\n",
    "@app.post(\"/generar_texto\", response_model=str)\n",
    "async def generar_texto(request: Request, form_data: Form(...)):\n",
    "    modelo_elegido = form_data.modelo\n",
    "    texto = form_data.texto\n",
    "\n",
    "    llm = OpenSourceLLM(modelo_elegido)\n",
    "    respuesta = llm.text2text(texto)\n",
    "\n",
    "    return respuesta\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "config = uvicorn.Config(app=app, host=\"127.0.0.1\", port=8000, loop='asyncio')\n",
    "server = uvicorn.Server(config)\n",
    "await server.serve()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [21076]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:62398 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:62403 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:62403 - \"GET /openapi.json HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, File, UploadFile, Query\n",
    "from fastapi.responses import JSONResponse, FileResponse\n",
    "from fastapi.encoders import jsonable_encoder\n",
    "import shutil\n",
    "from typing import List\n",
    "import subprocess\n",
    "from osllm import OpenSourceLLM\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "llm = OpenSourceLLM()\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    return {\"message\": 'MODELOS_CARGADOS = [\"llama2\": \"Llama2 (70B)\",\"mixtral\": \"Mixtral (35B)\",\"bakllava\": \"Bakllava (15B)\"]'}\n",
    "\n",
    "@app.post(\"/download_model/\")\n",
    "async def download_model(model: str = Query(..., title=\"Elección de modelo\", description=\"Elección de modelo a descargar desde el repositorio de Ollama\")):\n",
    "    try:\n",
    "        subprocess.run(['ollama', 'pull', model])\n",
    "        return JSONResponse(content=jsonable_encoder({\"message\": f\" Descargado el modelo {model} exitosamente\"}), status_code=200)\n",
    "    except Exception as e:\n",
    "        return JSONResponse(content=jsonable_encoder({\"error\": str(e)}), status_code=500)\n",
    "\n",
    "@app.get(\"/txt2txt/\")\n",
    "async def txt2txt(model: str = Query(..., title=\"Elección de modelo\", description=\"Elección de modelo con opciones en: llama2, mixtral\"),\n",
    "                  prompt: str = Query(..., title=\"Prompt\", description=\"Instrucción en la consulta al LLM\")):\n",
    "    try:\n",
    "        res = llm.text2text(prompt)\n",
    "        return JSONResponse(content=jsonable_encoder({\"message\": f\"{res}\"}), status_code=200)\n",
    "    except Exception as e:\n",
    "        return JSONResponse(content=jsonable_encoder({\"error\": str(e)}), status_code=500)\n",
    "\n",
    "@app.get(\"/speech-to-text/\")\n",
    "async def speech_to_text(speakers: str = Query(..., title=\"Speaker Name\", description=\"Name of the speaker\"),\n",
    "                         language: str = Query(..., title=\"Language\", description=\"Language for speech-to-text\")):\n",
    "    response_content = {\n",
    "        \"message\": f\"speech-to-text conversion for {speakers} in {language} language.\"\n",
    "    }\n",
    "    return response_content\n",
    "\n",
    "@app.get(\"/get-transcript/\")\n",
    "async def get_text_file(name_file: str = Query(..., title=\"Nombre del archivo\", description=\"Nombre del archivo\")):\n",
    "    file_path = f\"/workspace/transcript.txt\"\n",
    "    transcription_processor.run_transcription('/workspace/' + name_file)\n",
    "    return FileResponse(file_path, filename=f\"transcript.txt\", media_type=\"text/plain\")\n",
    "\n",
    "@app.get(\"/image-to-text/\")\n",
    "async def image_to_text(filename: str = Query(..., title=\"Nombre del archivo de imagen\", description=\"Nombre del archivo de imagen\"),\n",
    "                          prompt: str = Query(..., title=\"Image Prompt\", description=\"Description or prompt for the image\")):\n",
    "    \"\"\"Procesa una imagen y genera texto utilizando un modelo LLM.\"\"\"\n",
    "\n",
    "    self.rm_old_files()  # Clean up old files before processing\n",
    "\n",
    "    file_path = f\"img/{filename}\"\n",
    "    try:\n",
    "        pil_image = Image.open(file_path)\n",
    "        image_b64 = self.convert_to_base64(pil_image)\n",
    "        generated_text = self.img2text(filename, prompt)\n",
    "\n",
    "        response_content = {\n",
    "            \"message\": \"Image-to-text generation successful\",\n",
    "            \"generated_text\": generated_text\n",
    "        }\n",
    "        return response_content\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"message\": f\"Error al procesar la imagen: {e}\"}\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "config = uvicorn.Config(app=app, host=\"127.0.0.1\", port=8000, loop='asyncio')\n",
    "server = uvicorn.Server(config)\n",
    "await server.serve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
